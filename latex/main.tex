\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{braket}
\usepackage{authblk}
\renewcommand\Authfont{\bfseries}
\renewcommand\Affilfont{\small\ttfamily}

%\usepackage{indentfirst}
%\usepackage[hangul]{kotex}
\usepackage[
backend=biber,
style=numeric
]{biblatex}

\usepackage{hyperref} 
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{
    \textbf{QAMP 2025} \\        
    \vspace{0.5em}                            
    \small \textit{Enhancing Quantum Diffusion Models for Complex Image Generation}            
}



\addbibresource{bibliography.bib}

\author[1]{Jeongbin Jo}
% author
\author[2]{Santanam Wishal}

% info.
\affil[1]{Department of Physics \\ Yonsei University \\ \textit{jeongbin033@yonsei.ac.kr}}
\affil[2]{Asia Cyber University \\ \textit{santawishal17@gmail.com}}
\affil[3]{Eleven Dimension LLC \\ textit{shanjdk2012@gmail.com}

%\newcommand{\pref}[1]{(\ref{#1})}
\begin{document}
\maketitle
\tableofcontents
\begin{center}

%\vspace{-0.3in}
%\begin{tabular}{rl}
%Collaborators: & 
%\end{tabular}
\end{center}



\noindent
\rule{\linewidth}{0.4pt}
  
\section{Intrduction}
  \subsection{What’s the potential benefit of replacing neural network with PQCs?}

    \subsubsection*{Parameter Efficiency (Model Compression)}
    The authors state in their conclusion that "quantum diffusion models require fewer parameters with respect to their classical counterpart". 
    For example, their 'conditioned latent' model, which generated all 10 MNIST digits, used only 1110 parameters. 
    This is a massive reduction compared to the millions of parameters typical in classical U-Net architectures used for diffusion. 

    \subsubsection*{Expressivity and Scalability}
    The paper suggests that for a "full quantum case," it is "possible to generate distributions whose features scale exponentially with the number of qubits". 
    This implies PQCs have a fundamentally greater expressive power.


    \subsubsection*{Future Potential}
    The long-term benefit is the potential to "approximate probability distributions that are not classically tractable", including "quantum datasets" themselves.


  \subsection{How does the proposed QDM compare to just classical DM?}
    \subsubsection*{Hybrid Architecture}
    The paper's QDM is a hybrid model. 
    The forward process (adding noise) is performed classically. 
    The backward process (denoising) is performed by a PQC.


    \subsubsection*{PQC as a Denoising Operator}
    Instead of having the PQC predict the noise $\epsilon_{\theta}$ (like a classical U-Net), this model trains the PQC to be a direct denoising operator $P(\theta,t)$ such that **$P(\theta,t)|x_{t}\rangle=|x_{t-1}\rangle$**. 
    This avoids the "impractical" cycle of decoding/encoding quantum states at every timestep.


    \subsubsection*{Use of Complex Noise}
    Because PQC operations can make state coefficients complex, the authors found that using complex Gaussian noise ($\epsilon = \epsilon_r + i\epsilon_i$) during the classical forward process led to a "significant improvement in performance". 
    This is a notable departure from classical models.


    \subsubsection*{Comparable Performance (in Latent Space)}
    The paper's 'Latent QDM' (which uses a classical autoencoder first) achieved a competitive FID of 38.2 and was able to generate "samples of similar quality to those generated by classical algorithms".


\section{Diffusion Model}
  \subsection{Classical Diffusion Model}
  Diffusion models are a class of generative models in machine learning inspired by non-equilibrium thermodynamics. The goal is to learn a diffusion process that destroys structure in data, and then learn a reverse process that restores structure to generate new data samples from noise.\cite{diffusion-model}

    \subsubsection{Forward Process: Diffusion Process}
    The forward process is defined by a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule $\beta_1, \dots, \beta_T$.
    \begin{equation}
      q(x_t | x_{t-1}) = \mathcal{N}\left(x_t ; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf{I}\right)
    \end{equation}

    Using the notation $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$, we can sample $x_t$ at any arbitrary time step $t$ directly from $x_0$ in closed form:
    \begin{equation}
      q(x_t | x_0) = \mathcal{N}\left(x_t ; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)\mathbf{I}\right)
    \end{equation}
    This property allows for efficient training by eliminating the need to iterate through all previous timesteps.

    \subsubsection{Reverse Process: Generative Process}
    The generative process\cite{DBLP:journals/corr/abs-2006-11239} is defined as the reverse Markov chain. 
    Since the exact reverse posterior $q(x_{t-1}|x_t)$ is intractable, we approximate it using a neural network with parameters $\theta$:
    \begin{equation}
      p_\theta(x_{t-1} | x_t) = \mathcal{N}\left(x_{t-1} ; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t) \right)
    \end{equation}
    Here, $\mu_\theta(x_t, t)$ is the predicted mean, and $\Sigma_\theta(x_t, t)$ is the covariance (often fixed to $\beta_t \mathbf{I}$ or learned).

    \subsubsection{Loss Function}
    Training is performed by optimizing the variational lower bound (VLB) on the negative log-likelihood.
    \begin{equation}
      \mathcal{L} = \mathbb{E} \left[ - \log p_\theta(\mathbf{x}_0) \right] \leq \mathbb{E}_q \left[ - \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} | \mathbf{x}_0)} \right]
    \end{equation}

    This can be rewritten as a sum of KL divergence\cite{kl-divergence} terms:
    \begin{equation}
      \mathcal{L} = \mathbb{E}_q \left[ \underbrace{D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T))}_{L_T} + \sum_{t>1} \underbrace{D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0|\mathbf{x}_1)}_{L_0} \right]
    \end{equation}

    In practice, Ho et al. found that a simplified objective function yields better sample quality. The objective is to predict the noise $\epsilon$ added to $x_0$:
    \begin{equation}
      \mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t) \|^2 \right]
    \end{equation}
    where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and $\epsilon_\theta$ is a function approximator (e.g., U-Net).

    \subsubsection{Continuous-Time Formulation: SDE}
    The discrete diffusion process can be generalized to continuous time using Stochastic Differential Equations (SDEs). 
    The forward process is described by the following It\^{o} SDE\cite{Ito-calculus}:
    \begin{equation}
      d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w}
    \end{equation}
    where $\mathbf{f}(\cdot, t)$ is the drift coefficient, $g(t)$ is the diffusion coefficient, and $\mathbf{w}$ is a standard Wiener process.

    Remarkably, the reverse process—generating data from noise—is also a diffusion process governed by the \textit{reverse-time SDE}\cite{ANDERSON1982313}:
    \begin{equation}
      d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] dt + g(t) d\bar{\mathbf{w}}
    \end{equation}
    Here, $dt$ represents a negative time step, and $\bar{\mathbf{w}}$ is the Brownian motion in reverse time. 
    The term $\nabla_\mathbf{x} \log p_t(\mathbf{x})$, known as the \textit{score function}, points towards high-density regions of the data.

    Therefore, the core task of the diffusion model is to learn a score-based model 
    $s_\theta(\mathbf{x}, t) \approx \nabla_\mathbf{x} \log p_t(\mathbf{x})$ using a neural network (or PQC in our case), 
    allowing us to numerically solve the reverse SDE to generate samples from noise.

  \subsection{Quantum Diffusion Model}
  While classical diffusion models operate on probability distributions over classical data, 
  Quantum Diffusion Models (QDMs) extend this concept to the Hilbert space of quantum states. 
  The goal is to generate quantum states (density matrices) from a maximally mixed state.

    \subsubsection{Forward Process: Depolarizing Channel}
    Instead of adding Gaussian noise, the forward process in QDM is typically modeled as a standard depolarizing channel acting on a density matrix $\rho$. 
    For a system of $n$ qubits with dimension $d=2^n$, the state at step $t$ is given by:

    \begin{equation}
      \rho_t = \mathcal{E}_t(\rho_{t-1}) = (1 - p_t)\rho_{t-1} + p_t \frac{I}{d}
    \end{equation}

    where $p_t \in [0, 1]$ is the depolarization probability (noise schedule). 
    Similar to the classical case, we can express $\rho_t$ directly from the initial state $\rho_0$. 
    Let $\alpha_t = \prod_{s=1}^t (1 - p_s)$, then:

    \begin{equation}
      \rho_t = \alpha_t \rho_0 + (1 - \alpha_t) \frac{I}{d}
    \end{equation}

    As $t \to T$, $\alpha_T \to 0$, and the state converges to the maximally mixed state $\rho_T \approx \frac{I}{d}$, 
    which contains no information about $\rho_0$.

    \subsubsection{Reverse Process: Quantum Denoising}
    The reverse process aims to restore the quantum state from the noise. 
    This is modeled by a parameterized quantum circuit (PQC), denoted as a unitary operator $U(\theta)$. 
    The discrete reverse step can be approximated as:

    \begin{equation}
      \rho_{t-1} \approx \mathcal{D}_\theta(\rho_t, t) = U(\theta_t) \rho_t U^\dagger(\theta_t)
    \end{equation}

    For more complex generative tasks, the reverse process may involve ancillary qubits and measurements to simulate non-unitary maps.

    \subsubsection{Loss Function: Fidelity or Trace Distance}
    Unlike the KL divergence used in classical models, 
    quantum models often use Fidelity or Hilbert-Schmidt distance to measure the closeness between the generated state and the target state. 
    A common objective is to maximize the overlap with the target pure state $\ket{\psi}$:

    \begin{equation}
      \mathcal{L}(\theta) = 1 - \mathbb{E} \left[ \bra{\psi} \rho_{\text{gen}}(\theta) \ket{\psi} \right]
    \end{equation}
    Alternatively, for density matrices, we minimize the trace distance or maximize the quantum fidelity $F(\rho, \sigma) = (\text{tr}\sqrt{\sqrt{\rho}\sigma\sqrt{\rho}})^2$.


\section{Quantum Data Encoding}
We referred to IBM Quantum Platform, especially Quantum Machine Learning.\cite{Data-encoding}
  \subsection{Basis Encoding}
  Basis encoding maps a classical $P$-bit string directly to a computational basis state of a $P$-qubit system. 
  For a single feature represented as binary bits $(b_1, b_2, \dots, b_P)$, the quantum state is:
  \begin{equation}
    \ket{x} = \ket{b_1, b_2, \dots, b_P}, \quad b_i \in \{0, 1\}
  \end{equation}

  \subsection{Amplitude Encoding}
  Amplitude encoding maps a normalized classical $N$-dimensional data vector $\vec{x}$ to the amplitudes of an $n$-qubit quantum state, 
  where $n = \lceil \log_2 N \rceil$.

  \begin{equation}
    \ket{\psi_x} = \frac{1}{\alpha}\sum_{i=1}^N x_i \ket{i}
  \end{equation}
  Here, $\ket{i}$ is the computational basis state and $\alpha$ is a normalization constant ensuring $\langle \psi_x | \psi_x \rangle = 1$. 
  And $\alpha$ is a normalization constant to be determined from the data being encoded.

  \begin{equation}
    \alpha = \sqrt{\sum_{i=1}^{N}|x_i|^2}
  \end{equation}

  \subsection{Angle Encoding}
  Angle encoding maps each feature $x_k$ to the rotation angle of a qubit using $R_Y$ gates. 
  For a data vector $\vec{x}$, the state is a product state:
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^N R_Y(x_k)\ket{0} = 
    \bigotimes_{k=1}^N \left( \cos\left(\frac{x_k}{2}\right)\ket{0} + \sin\left(\frac{x_k}{2}\right)\ket{1} \right)
  \end{equation} 
  \subsection{Phase Encoding}
  Phase encoding maps data features to the phase of qubits using Phase gates $P(\phi)$, typically applied after Hadamard gates.
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^N P(x_k)\ket{+} = 
    \frac{1}{\sqrt{2^N}} \bigotimes_{k=1}^N \left( \ket{0} + e^{i x_k}\ket{1} \right)  
  \end{equation} 

  \subsection{Dense Angle Encoding}
  Dense angle encoding encodes two features, $x_k$ and $x_l$, into a single qubit using both $Y$-axis and $Z$-axis rotations.
  \begin{equation}
    \ket{x_k, x_l} = R_Z(x_l) R_Y(x_k)\ket{0} = \cos\left(\frac{x_k}{2}\right)\ket{0} + e^{i x_l} \sin\left(\frac{x_k}{2}\right)\ket{1} 
  \end{equation}

  Extending this to more features, the data vector $\vec{x} = (x_1, ..., x_N)$ can be encoded as:
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^{N/2}\left( \cos{x_{2k-1}}\ket{0} + e^{ix_{2k}}\sin{x_{2k-1}\ket{1}} \right)
  \end{equation}

\subsection{Architecture}
The proposed model, illustrated in Figure \ref{fig:architecture}, 
adopts a hybrid quantum-classical U-Net architecture inspired by recent advances in quantum generative diffusion models\cite{cacioppo2023quantumdiffusionmodels, qdm}. 
The architecture is designed to leverage the expressivity of quantum circuits while maintaining the reconstruction capability of classical networks. 
It consists of three distinct modules: a classical encoder, a quantum bottleneck (core), and a classical decoder.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figure/QuantumUNet_Final_LR.png}
  \caption{Schematic of the Hybrid Quantum-Classical U-Net Architecture. The model integrates a quantum bottleneck for feature extraction and employs a skip connection to preserve original semantic information during the reverse diffusion process.}
  \label{fig:architecture}
\end{figure}

First, the \textbf{Classical Encoder} transforms the input data (flattened vector) into a lower-dimensional latent representation. 
To seamlessly interface with the quantum layer, we utilize complex-valued linear layers (\texttt{ComplexLinear}) that preserve phase information necessary for quantum state preparation.

Second, the \textbf{Quantum Bottleneck} acts as the core generative component. 
We employ \textit{amplitude encoding} to efficiently map the classical latent vector $\mathbf{z}$ onto an $n$-qubit quantum state $|\psi\rangle$ by normalizing the complex vector. 
This state is evolved by a Parameterized Quantum Circuit (PQC), $U(\theta)$, which learns the reverse diffusion dynamics. 
The resulting quantum state is then measured to extract quantum features (Q-Feats).

Finally, the \textbf{Classical Decoder} reconstructs the denoised sample from the quantum features. 
A crucial design element is the inclusion of a \textbf{Skip Connection} (red path in Figure \ref{fig:architecture}). 
This connection concatenates the original flattened input vector directly with the quantum features in the decoder. 
This mechanism is essential for mitigating the "bottleneck" problem inherent in low-qubit quantum circuits and preventing the model from converging to trivial identity mappings by preserving the original signal structure.

\section{Experimental Results}

We evaluated the proposed Hybrid Quantum U-Net using the MNIST dataset. 
The training was conducted for 100 epochs, and the model demonstrated stable convergence behavior. 
As shown in Figure \ref{fig:loss}, the final infidelity loss reached approximately 0.25. 
Remarkably, the total execution time was only about 3 minutes, highlighting the computational efficiency of our hybrid architecture compared to fully classical counterparts or more complex quantum simulations.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figure/loss.png}
  \caption{Training Loss Convergence. The model achieves a stable loss of $\approx 0.25$ within 100 epochs, demonstrating rapid convergence.}
  \label{fig:loss}
\end{figure}

Figure \ref{fig:forward} visualizes the forward diffusion process used in our experiment. 
The input data is gradually corrupted by adding Gaussian noise over discrete time steps $t$, eventually converging to an isotropic Gaussian distribution.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figure/forward.png}
  \caption{Visualization of the Forward Diffusion Process. The original digits (left) are progressively transformed into random noise (right).}
  \label{fig:forward}
\end{figure}

To analyze the generative capability and scalability of the model, 
we compared two scenarios: a low-complexity subset (digits 0 and 1) and the full dataset (digits 0 through 9). 
The generation results are presented in Figure \ref{fig:gen}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/trial1.png}
    \caption{Binary Class (Digits 0, 1)} 
    \label{fig:trial1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/trial2.png}
    \caption{Multi-Class (Digits 0-9)} 
    \label{fig:trial2}
  \end{subfigure}
  \caption{Comparison of Generated Samples. (a) shows sharp features for the binary case, while (b) exhibits increased ambiguity and blurring due to higher data complexity.}
  \label{fig:gen}
\end{figure}

As observed in Figure \ref{fig:trial1}, the model successfully generates distinct and sharp images when trained on binary classes. 
However, as the complexity increases to include all ten digits (Figure \ref{fig:trial2}), the generated samples exhibit ambiguous shapes and blurring. 
This suggests that while the current quantum bottleneck is efficient, its limited Hilbert space (number of qubits) constrains the expressibility required to fully capture the complex multimodal distribution of the entire dataset.


\section{Conclusion}


\printbibliography

\end{document}
