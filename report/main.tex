\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{braket}
\usepackage{authblk}
\renewcommand\Authfont{\bfseries}
\renewcommand\Affilfont{\small\ttfamily}

%\usepackage{indentfirst}
%\usepackage[hangul]{kotex}
\usepackage[
backend=biber,
style=numeric
]{biblatex}

\usepackage{hyperref} 
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{
    \textbf{QAMP 2025} \\        
    \vspace{0.5em}                            
    \small \textit{Enhancing Quantum Diffusion Models for Complex Image Generation}            
}



\addbibresource{bibliography.bib}

% author
\author[1]{Jeongbin Jo}
\author[2]{Santanam Wishal}
\author[3]{Shah Md Khalil Ullah}
\author[4]{Shan Kowalski}

% info.
\affil[1]{Department of Physics \\ Yonsei University \\ \textit{jeongbin033@yonsei.ac.kr}}
\affil[2]{Asia Cyber University \\ \textit{santawishal17@gmail.com}}
\affil[3]{Khulna University of Engineering and Technology \\ \textit{hamimkhandakar222@gmail.com}}
\affil[4]{Eleven Dimension LLC \\ \textit{shanjdk2012@gmail.com}}

%\newcommand{\pref}[1]{(\ref{#1})}
\begin{document}
\maketitle
\begin{abstract}
  In this study, we propose a Hybrid Quantum-Classical U-Net architecture designed for complex image generation.
  By integrating a classical encoder-decoder structure with a Quantum Bottleneck, 
  the model leverages the expressivity of quantum circuits while maintaining structural efficiency. 
  The architecture employs Skip Connections to preserve semantic information, which is critical for maintaining image sharpness.
\end{abstract}

\tableofcontents
\begin{center}

%\vspace{-0.3in}
%\begin{tabular}{rl}
%Collaborators: & 
%\end{tabular}
\end{center}



\noindent
\rule{\linewidth}{0.4pt}



  
\section{Intrduction}
  \subsection{Quantum Machine Learning}

  \subsection{Quantum Convolutional Neural Network}
  


\section{Diffusion Model}
  \subsection{Classical Diffusion Model}
  Diffusion models are a class of generative models in machine learning inspired by non-equilibrium thermodynamics. The goal is to learn a diffusion process that destroys structure in data, and then learn a reverse process that restores structure to generate new data samples from noise.\cite{diffusion-model}

    \subsubsection{Forward Process: Diffusion Process}
    The forward process is defined by a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule $\beta_1, \dots, \beta_T$.
    \begin{equation}
      q(x_t | x_{t-1}) = \mathcal{N}\left(x_t ; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf{I}\right)
    \end{equation}

    Using the notation $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$, we can sample $x_t$ at any arbitrary time step $t$ directly from $x_0$ in closed form:
    \begin{equation}
      q(x_t | x_0) = \mathcal{N}\left(x_t ; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)\mathbf{I}\right)
    \end{equation}
    This property allows for efficient training by eliminating the need to iterate through all previous timesteps.

    \subsubsection{Reverse Process: Generative Process}
    The generative process\cite{DBLP:journals/corr/abs-2006-11239} is defined as the reverse Markov chain. 
    Since the exact reverse posterior $q(x_{t-1}|x_t)$ is intractable, we approximate it using a neural network with parameters $\theta$:
    \begin{equation}
      p_\theta(x_{t-1} | x_t) = \mathcal{N}\left(x_{t-1} ; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t) \right)
    \end{equation}
    Here, $\mu_\theta(x_t, t)$ is the predicted mean, and $\Sigma_\theta(x_t, t)$ is the covariance (often fixed to $\beta_t \mathbf{I}$ or learned).

    \subsubsection{Loss Function}
    Training is performed by optimizing the variational lower bound (VLB) on the negative log-likelihood.
    \begin{equation}
      \mathcal{L} = \mathbb{E} \left[ - \log p_\theta(\mathbf{x}_0) \right] \leq \mathbb{E}_q \left[ - \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} | \mathbf{x}_0)} \right]
    \end{equation}

    This can be rewritten as a sum of KL divergence\cite{kl-divergence} terms:
    \begin{equation}
      \mathcal{L} = \mathbb{E}_q \left[ \underbrace{D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T))}_{L_T} + \sum_{t>1} \underbrace{D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0|\mathbf{x}_1)}_{L_0} \right]
    \end{equation}

    In practice, Ho et al. found that a simplified objective function yields better sample quality. The objective is to predict the noise $\epsilon$ added to $x_0$:
    \begin{equation}
      \mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t) \|^2 \right]
    \end{equation}
    where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and $\epsilon_\theta$ is a function approximator (e.g., U-Net).

    \subsubsection{Continuous-Time Formulation: SDE}
    The discrete diffusion process can be generalized to continuous time using Stochastic Differential Equations (SDEs). 
    The forward process is described by the following It\^{o} SDE\cite{Ito-calculus}:
    \begin{equation}
      d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w}
    \end{equation}
    where $\mathbf{f}(\cdot, t)$ is the drift coefficient, $g(t)$ is the diffusion coefficient, and $\mathbf{w}$ is a standard Wiener process.

    Remarkably, the reverse process—generating data from noise—is also a diffusion process governed by the \textit{reverse-time SDE}\cite{ANDERSON1982313}:
    \begin{equation}
      d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] dt + g(t) d\bar{\mathbf{w}}
    \end{equation}
    Here, $dt$ represents a negative time step, and $\bar{\mathbf{w}}$ is the Brownian motion in reverse time. 
    The term $\nabla_\mathbf{x} \log p_t(\mathbf{x})$, known as the \textit{score function}, points towards high-density regions of the data.

    Therefore, the core task of the diffusion model is to learn a score-based model 
    $s_\theta(\mathbf{x}, t) \approx \nabla_\mathbf{x} \log p_t(\mathbf{x})$ using a neural network (or PQC in our case), 
    allowing us to numerically solve the reverse SDE to generate samples from noise.

  \subsection{Quantum Diffusion Model}
  While classical diffusion models operate on probability distributions over classical data, 
  Quantum Diffusion Models (QDMs) extend this concept to the Hilbert space of quantum states. 
  The goal is to generate quantum states (density matrices) from a maximally mixed state.

    \subsubsection{Forward Process: Depolarizing Channel}
    Instead of adding Gaussian noise, the forward process in QDM is typically modeled as a standard depolarizing channel acting on a density matrix $\rho$. 
    For a system of $n$ qubits with dimension $d=2^n$, the state at step $t$ is given by:

    \begin{equation}
      \rho_t = \mathcal{E}_t(\rho_{t-1}) = (1 - p_t)\rho_{t-1} + p_t \frac{I}{d}
    \end{equation}

    where $p_t \in [0, 1]$ is the depolarization probability (noise schedule). 
    Similar to the classical case, we can express $\rho_t$ directly from the initial state $\rho_0$. 
    Let $\alpha_t = \prod_{s=1}^t (1 - p_s)$, then:

    \begin{equation}
      \rho_t = \alpha_t \rho_0 + (1 - \alpha_t) \frac{I}{d}
    \end{equation}

    As $t \to T$, $\alpha_T \to 0$, and the state converges to the maximally mixed state $\rho_T \approx \frac{I}{d}$, 
    which contains no information about $\rho_0$.

    \subsubsection{Reverse Process: Quantum Denoising}
    The reverse process aims to restore the quantum state from the noise. 
    This is modeled by a parameterized quantum circuit (PQC), denoted as a unitary operator $U(\theta)$. 
    The discrete reverse step can be approximated as:

    \begin{equation}
      \rho_{t-1} \approx \mathcal{D}_\theta(\rho_t, t) = U(\theta_t) \rho_t U^\dagger(\theta_t)
    \end{equation}

    For more complex generative tasks, the reverse process may involve ancillary qubits and measurements to simulate non-unitary maps.

    \subsubsection{Loss Function: Fidelity or Trace Distance}
    Unlike the KL divergence used in classical models, 
    quantum models often use Fidelity or Hilbert-Schmidt distance to measure the closeness between the generated state and the target state. 
    A common objective is to maximize the overlap with the target pure state $\ket{\psi}$:

    \begin{equation}
      \mathcal{L}(\theta) = 1 - \mathbb{E} \left[ \bra{\psi} \rho_{\text{gen}}(\theta) \ket{\psi} \right]
    \end{equation}
    Alternatively, for density matrices, we minimize the trace distance or maximize the quantum fidelity $F(\rho, \sigma) = (\text{tr}\sqrt{\sqrt{\rho}\sigma\sqrt{\rho}})^2$.


\section{Quantum Data Encoding}
We referred to IBM Quantum Platform, especially Quantum Machine Learning.\cite{Data-encoding}
  \subsection{Basis Encoding}
  Basis encoding maps a classical $P$-bit string directly to a computational basis state of a $P$-qubit system. 
  For a single feature represented as binary bits $(b_1, b_2, \dots, b_P)$, the quantum state is:
  \begin{equation}
    \ket{x} = \ket{b_1, b_2, \dots, b_P}, \quad b_i \in \{0, 1\}
  \end{equation}

  \subsection{Amplitude Encoding}
  Amplitude encoding maps a normalized classical $N$-dimensional data vector $\vec{x}$ to the amplitudes of an $n$-qubit quantum state, 
  where $n = \lceil \log_2 N \rceil$.

  \begin{equation}
    \ket{\psi_x} = \frac{1}{\alpha}\sum_{i=1}^N x_i \ket{i}
  \end{equation}
  Here, $\ket{i}$ is the computational basis state and $\alpha$ is a normalization constant ensuring $\langle \psi_x | \psi_x \rangle = 1$. 
  And $\alpha$ is a normalization constant to be determined from the data being encoded.

  \begin{equation}
    \alpha = \sqrt{\sum_{i=1}^{N}|x_i|^2}
  \end{equation}

  \subsection{Angle Encoding}
  Angle encoding maps each feature $x_k$ to the rotation angle of a qubit using $R_Y$ gates. 
  For a data vector $\vec{x}$, the state is a product state:
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^N R_Y(x_k)\ket{0} = 
    \bigotimes_{k=1}^N \left( \cos\left(\frac{x_k}{2}\right)\ket{0} + \sin\left(\frac{x_k}{2}\right)\ket{1} \right)
  \end{equation} 
  \subsection{Phase Encoding}
  Phase encoding maps data features to the phase of qubits using Phase gates $P(\phi)$, typically applied after Hadamard gates.
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^N P(x_k)\ket{+} = 
    \frac{1}{\sqrt{2^N}} \bigotimes_{k=1}^N \left( \ket{0} + e^{i x_k}\ket{1} \right)  
  \end{equation} 

  \subsection{Dense Angle Encoding}
  Dense angle encoding encodes two features, $x_k$ and $x_l$, into a single qubit using both $Y$-axis and $Z$-axis rotations.
  \begin{equation}
    \ket{x_k, x_l} = R_Z(x_l) R_Y(x_k)\ket{0} = \cos\left(\frac{x_k}{2}\right)\ket{0} + e^{i x_l} \sin\left(\frac{x_k}{2}\right)\ket{1} 
  \end{equation}

  Extending this to more features, the data vector $\vec{x} = (x_1, ..., x_N)$ can be encoded as:
  \begin{equation}
    \ket{\vec{x}} = \bigotimes_{k=1}^{N/2}\left( \cos{x_{2k-1}}\ket{0} + e^{ix_{2k}}\sin{x_{2k-1}\ket{1}} \right)
  \end{equation}

\subsection{Architecture}
The proposed model, illustrated in Figure \ref{fig:architecture}, 
adopts a hybrid quantum-classical U-Net architecture inspired by recent advances in quantum generative diffusion models\cite{cacioppo2023quantumdiffusionmodels, qdm}. 
The architecture is designed to leverage the expressivity of quantum circuits while maintaining the reconstruction capability of classical networks. 
It consists of three distinct modules: a classical encoder, a quantum bottleneck (core), and a classical decoder.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figure/QuantumUNet_Final_LR.png}
  \caption{Schematic of the Hybrid Quantum-Classical U-Net Architecture. The model integrates a quantum bottleneck for feature extraction and employs a skip connection to preserve original semantic information during the reverse diffusion process.}
  \label{fig:architecture}
\end{figure}

First, the \textbf{Classical Encoder} transforms the input data (flattened vector) into a lower-dimensional latent representation. 
To seamlessly interface with the quantum layer, we utilize complex-valued linear layers (\texttt{ComplexLinear}) that preserve phase information necessary for quantum state preparation.

Second, the \textbf{Quantum Bottleneck} acts as the core generative component. 
We employ \textit{amplitude encoding} to efficiently map the classical latent vector $\mathbf{z}$ onto an $n$-qubit quantum state $|\psi\rangle$ by normalizing the complex vector. 
This state is evolved by a Parameterized Quantum Circuit (PQC), $U(\theta)$, which learns the reverse diffusion dynamics. 
The resulting quantum state is then measured to extract quantum features (Q-Feats).

Finally, the \textbf{Classical Decoder} reconstructs the denoised sample from the quantum features. 
A crucial design element is the inclusion of a \textbf{Skip Connection} (red path in Figure \ref{fig:architecture}). 
This connection concatenates the original flattened input vector directly with the quantum features in the decoder. 
This mechanism is essential for mitigating the "bottleneck" problem inherent in low-qubit quantum circuits and preventing the model from converging to trivial identity mappings by preserving the original signal structure.

\section{Experimental Results}

We evaluated the proposed Hybrid Quantum U-Net using the MNIST dataset. 
The training was conducted for 100 epochs, and the model demonstrated stable convergence behavior. 
As shown in Figure \ref{fig:loss}, the final infidelity loss reached approximately 0.25. 
Remarkably, the total execution time was only about 3 minutes, highlighting the computational efficiency of our hybrid architecture compared to fully classical counterparts or more complex quantum simulations.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figure/loss.png}
  \caption{Training Loss Convergence. The model achieves a stable loss of $\approx 0.25$ within 100 epochs, demonstrating rapid convergence.}
  \label{fig:loss}
\end{figure}

Figure \ref{fig:forward} visualizes the forward diffusion process used in our experiment. 
The input data is gradually corrupted by adding Gaussian noise over discrete time steps $t$, eventually converging to an isotropic Gaussian distribution.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figure/forward.png}
  \caption{Visualization of the Forward Diffusion Process. The original digits (left) are progressively transformed into random noise (right).}
  \label{fig:forward}
\end{figure}

To analyze the generative capability and scalability of the model, 
we compared two scenarios: a low-complexity subset (digits 0 and 1) and the full dataset (digits 0 through 9). 
The generation results are presented in Figure \ref{fig:gen}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/trial1.png}
    \caption{Binary Class (Digits 0, 1)} 
    \label{fig:trial1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure/trial2.png}
    \caption{Multi-Class (Digits 0-9)} 
    \label{fig:trial2}
  \end{subfigure}
  \caption{Comparison of Generated Samples. (a) shows sharp features for the binary case, while (b) exhibits increased ambiguity and blurring due to higher data complexity.}
  \label{fig:gen}
\end{figure}

As observed in Figure \ref{fig:trial1}, the model successfully generates distinct and sharp images when trained on binary classes. 
However, as the complexity increases to include all ten digits (Figure \ref{fig:trial2}), the generated samples exhibit ambiguous shapes and blurring. 
This suggests that while the current quantum bottleneck is efficient, its limited Hilbert space (number of qubits) constrains the expressibility required to fully capture the complex multimodal distribution of the entire dataset.


\section{Conclusion}
In this study, we proposed a \textbf{Hybrid Quantum-Classical U-Net} architecture 
for complex image generation and validated its effectiveness using the MNIST dataset. 
The experimental results demonstrated that the proposed model exhibits significant generative capabilities with a reduced number of parameters, confirming the potential of hybrid structures in the NISQ era. 
The key insights gained, technical challenges faced, and our current research direction are summarized below.

\subsection*{Key Insights and Learnings}
\begin{itemize}
    \item \textbf{Structural Efficiency:} 
    Through code implementation (\texttt{QuantumUNet}), we verified that the `Quantum Bottleneck' architecture—which compresses features via a classical encoder before mapping them to a quantum state is significantly more efficient than encoding the entire high-dimensional image directly into qubits.
    \item \textbf{Importance of Skip Connections:} 
    We discovered that \textbf{Skip Connections}, which combine original classical features with quantum features in the decoder stage, are critical. Without them, the information loss inherent in the quantum circuit significantly degrades image sharpness. 
    This highlights that preserving the \textbf{Classical Path} is essential for maintaining reconstruction quality in hybrid models.
\end{itemize}

\subsection*{Technical Challenges}
\begin{itemize}
    \item \textbf{Simulation Overhead (Running Speed):} 
    A major practical barrier was the \textbf{ML running speed}. Simulating quantum circuits (especially tensor product operations for entanglement) on classical hardware incurs massive computational costs. 
    This overhead severely constrained our ability to scale up the number of qubits and circuit depth within a reasonable training time.
    \item \textbf{Barren Plateaus \& Optimization Landscape:} 
    Beyond computational speed, we faced the fundamental challenge of \textbf{Barren Plateaus}. 
    The standard Adam optimizer struggled to navigate the flat loss landscapes typical of parameterized quantum circuits (PQCs). 
    This often led to stagnation or convergence to local minima, particularly when attempting to learn the complex multi-modal distributions of the multi-class MNIST dataset.
\end{itemize}

\subsection*{Current Status: Enhancing Optimization with QNGD}
To address these optimization challenges and ensure convergence stability, we are currently implementing \textbf{Quantum Natural Gradient Descent (QNGD)}.

\begin{itemize}
    \item \textbf{Addressing the Optimization Geometry:} 
    Standard optimizers like Adam assume a Euclidean parameter space, which is ill-suited for the quantum state space that follows Riemannian geometry. 
    We are adopting QNGD to adjust the gradient direction by incorporating the \textbf{Fubini-Study Metric Tensor} ($g_{ij}$), which represents the curvature of the quantum information geometry. 
    The update rule is defined as:
    \begin{equation}
        \theta_{t+1} = \theta_t - \eta g^{-1}(\theta_t) \nabla \mathcal{L}(\theta_t)
    \end{equation}
    where $\eta$ is the learning rate and $g^{-1}(\theta_t)$ corrects the gradient based on the local curvature.
    
    \item \textbf{Quality over Speed (Trade-off):} We acknowledge that calculating the metric tensor increases the computational cost per step, potentially lengthening the wall-clock training time. 
    However, we accept this trade-off to \textbf{mitigate Barren Plateaus} and achieve higher \textbf{Fidelity} with fewer training epochs. 
    Our focus has shifted from raw simulation speed to \textbf{`Optimization Efficiency'}, aiming to overcome the expressivity and convergence limitations of the current model.
\end{itemize}


\printbibliography

\end{document}
